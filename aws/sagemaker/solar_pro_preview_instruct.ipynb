{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Friendli Container: Solar Pro Preview Instruct Model Package from AWS Marketplace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Friendli Container: Solar Pro Preview Instruct** is a SageMaker model package of the [Friendli Container](https://friendli.ai/products/container) with an instruction-tuned 8 billion parameter language model from Meta.\n",
    "\n",
    "This sample notebook demonstrates how to deploy the [Friendli Container: Solar Pro Preview Instruct](https://aws.amazon.com/marketplace/pp/prodview-fmmknqim7hr6s) on Amazon SageMaker.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements that render correctly in the Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or from Amazon SageMaker Studio.\n",
    "2. Ensure that the IAM role used for the instance has the **AmazonSageMakerFullAccess** permission.\n",
    "3. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have the authority to make AWS Marketplace subscriptions through your AWS account: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        2. **aws-marketplace:Unsubscribe**\n",
    "        3. **aws-marketplace:Subscribe**\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create the input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Perform real-time streaming inference](#D.-Perform-real-time-streaming-inference)\n",
    "   5. [Delete the endpoint](#D.-Delete-the-endpoint)\n",
    "3. [Perform batch inference](#3.-Perform-batch-inference) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time by hitting `Shift+Enter` to run a cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page: [Friendli Container: Solar Pro Preview Instruct](https://aws.amazon.com/marketplace/pp/prodview-fmmknqim7hr6s)\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with the EULA, pricing, and the support terms. \n",
    "1. Once you click on the **Continue to configuration** button and choose your **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 sagemaker sseclient-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = \"arn:aws:sagemaker:us-east-1:172947302787:model-package/friendli-container-v1-6-11-llama-3-1-8b-instruct-int8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sseclient\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage, get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line could raise an error if youâ€™re trying to execute this notebook from somewhere other than SageMaker(e.g. a local environment).\n",
    "# If an error occurs, change the execution role to \"<ARN OF ROLE WITH SageMakerFullAccess>\".\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [this documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"friendli-container-solar-pro-preview-instruct\"\n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "real_time_inference_instance_type = \"ml.g5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "endpoint_name = model_name\n",
    "\n",
    "# Deploy the model\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create the input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request/response payloads are compatible with the OpenAI chat completion endpoint.\n",
    "\n",
    "The input payload is composed of:\n",
    "- **messages**(**required**, list of objects): A list of messages comprising the conversation so far.\n",
    "  - **role**(**required**, [`system`, `user`, `assistant`, `tool`]): The role of the messages author.\n",
    "  - **content**(**required**, string): The content of message\n",
    "- frequency_penalty(float): Number between -2.0 and 2.0. Positive values penalizes tokens that have been sampled, taking into account their frequency in the preceding text. This penalization diminishes the model's tendency to reproduce identical lines verbatim.\n",
    "- presence_penalty(float): Penalizes tokens that have already appeared in the generated result (plus the input tokens for decoder-only models). should be greater than or equal to 1.0. 1.0 means no penalty. see keskar et al., 2019 for more details. this is similar to Hugging Face's [repetition_penalty](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.repetition_penalty) argument.\n",
    "- max_tokens(integer): The maximum number of tokens to generate. For decoder-only models like GPT, the length of your input tokens plus `max_tokens` should not exceed the model's maximum length (e.g., 2048 for OpenAI GPT-3). For encoder-decoder models like T5 or BlenderBot, `max_tokens` should not exceed the model's maximum output length. This is similar to Hugging Face's [max_new_tokens](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.max_new_tokens) argument.\n",
    "- min_tokens(integer): The minimum number of tokens to generate. default value is 0. this is similar to hugging face's [min_new_tokens](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.min_new_tokens) argument.\n",
    "- n(integer): The number of independently generated results for the prompt. Not supported when using beam search. Defaults to 1. This is similar to Hugging Face's [num_return_sequences](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.num_return_sequences) argument.\n",
    "- stop(list of strings): When one of the stop phrases appears in the generation result, the API will stop generation. The stop phrases are excluded from the result. Defaults to empty list.\n",
    "- stream(boolean): Whether to stream generation result. When set true, each token will be sent as [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) once generated.\n",
    "- temperature(float): Sampling temperature. Smaller temperature makes the generation result closer to greedy, argmax (i.e., `top_k = 1`) sampling. defaults to 1.0. this is similar to hugging face's [temperature argument](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.generationconfig.temperature).\n",
    "- top_p(float): Tokens comprising the top `top_p` probability mass are kept for sampling. Numbers between 0.0 (exclusive) and 1.0 (inclusive) are allowed. Defaults to 1.0. This is similar to Hugging Face's [top_p](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.top_p) argument.\n",
    "- top_k(integer): The number of highest probability tokens to keep for sampling. Numbers between 0 and the vocab size of the model (both inclusive) are allowed. The default value is 0, which means that the API does not apply top-k filtering. This is similar to Hugging Face's [top_k](https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/text_generation#transformers.GenerationConfig.top_k) argument.\n",
    "- timeout_microseconds(integer): Request timeout. Gives the `HTTP 429 Too Many Requests` response status code. Default behavior is no timeout.\n",
    "- seed(list of integers): Seed to control random procedure. If nothing is given, random seed is used for sampling, and return the seed along with the generated result. When using the `n` argument, you can pass a list of seed values to control all of the independent generations.\n",
    "- eos_token(list of integers): A list of endpoint sentence tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Input payload example\n",
    "input_payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Hello!\"\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"max_tokens\": 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint(endpoint_name, payload):\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    return response['Body'].read().decode('utf-8')\n",
    "\n",
    "response = json.loads(invoke_endpoint(endpoint_name, input_payload))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Perform real-time streaming inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_streaming_endpoint(endpoint_name, payload):\n",
    "    response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    event_stream = response['Body']\n",
    "    for event in event_stream:\n",
    "        yield event[\"PayloadPart\"][\"Bytes\"]\n",
    "\n",
    "input_payload['stream'] = True\n",
    "response = invoke_streaming_endpoint(endpoint_name, input_payload)\n",
    "client = sseclient.SSEClient(response)\n",
    "\n",
    "for event in client.events():\n",
    "    if event.data == \"[DONE]\":\n",
    "        break\n",
    "    data = json.loads(event.data)\n",
    "    if data.get(\"choices\"):\n",
    "        print(data[\"choices\"][0][\"delta\"].get(\"content\", \"\"), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're finished with the real-time inference, you can terminate the endpoint to avoid extra charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
